{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb117e5",
   "metadata": {},
   "source": [
    "# Ore Proximity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120f7f0",
   "metadata": {},
   "source": [
    "A consultant is using metal abundance changes to predict proximity to an orebody.  Samples are classified as proximal (A) or distal (B) based on Euclidean distance to a wireframe model of the orebody.\n",
    "\n",
    "1. Can we use the same geochemical data and labels to generate a predictive model for future drill holes which can label samples on whether they are in class A or class B?\n",
    "2. More data has been acquired since the geochemist completed her work - can we predict labels onto these data points (labelled “?”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ccd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder \n",
    "import seaborn as sns\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e051ac5",
   "metadata": {},
   "source": [
    "## Step 1: Data - QA/QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e738e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data_for_distribution.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a97680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701bd91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check how many NaN values are in each column\n",
    "nan_values = data.isna().sum()\n",
    "\n",
    "# Print the number of NaN values per column\n",
    "print(\"Number of NaN values per column:\")\n",
    "print(nan_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9369f",
   "metadata": {},
   "source": [
    "Looking at the data, I saw that there are some there are some QA/QC issues.\n",
    "These included\n",
    "- Au data type is object\n",
    "- missing values\n",
    "- values below the detection limit (<0.005)\n",
    "- unsuitable values (-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943cad16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "elements = ['As', 'Au', 'Pb', 'Fe', 'Mo', 'Cu', 'S', 'Zn']\n",
    "\n",
    "# Replace '<0.005' with half of the detection limit and '-999' with NaN\n",
    "data[elements] = data[elements].replace({'<0.005': 0.005 / 2, '-999': np.nan})\n",
    "\n",
    "# Convert the 'Au' column to float64\n",
    "data['Au'] = pd.to_numeric(data['Au'], errors='coerce')\n",
    "\n",
    "# Drop NaN values\n",
    "data_cleaned = data.dropna(subset=elements)\n",
    "\n",
    "data_cleaned.to_csv(\"./data/cleaned_data.csv\", index=False)\n",
    "data_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some checks:\n",
    "\n",
    "# Check the data type of 'Au' column\n",
    "print(data['Au'].dtype)\n",
    "\n",
    "nan_check = data_cleaned.isna().sum()\n",
    "\n",
    "# Print NaN values if any exist\n",
    "if nan_check.any():\n",
    "    print(\"NaN values in cleaned data:\")\n",
    "    print(nan_check[nan_check > 0], \"\\n\")\n",
    "\n",
    "for element in elements:\n",
    "    # Check for values equal to '-999'\n",
    "    if (data_cleaned[element] == -999).any():\n",
    "        print(f\"Warning: '{element}' contains '-999' values.\")\n",
    "    \n",
    "    # Check for values equal to '<0.005'\n",
    "    if (data_cleaned[element] == '<0.005').any():\n",
    "        print(f\"Warning: '{element}' contains '<0.005' values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c5522b",
   "metadata": {},
   "source": [
    "We can do other data checks but I am assuming these are the only issues and moving on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2aef0f",
   "metadata": {},
   "source": [
    "## Step 2: Modelling\n",
    "### Aim 1: Generate a predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d810e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the distribution of classes\n",
    "class_counts = data_cleaned['Class'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945df09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mostly from https://www.datacamp.com/tutorial/random-forests-classifier-python\n",
    "\n",
    "# Separate labeled and unlabeled data\n",
    "labeled_data = data_cleaned[data_cleaned['Class'].isin(['A', 'B'])]\n",
    "unlabeled_data = data_cleaned[data_cleaned['Class'] == '?']\n",
    "\n",
    "# Features and target\n",
    "X = labeled_data[elements]\n",
    "y = labeled_data['Class']\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#Hyperparameter tuning \n",
    "param_dist = {'n_estimators': randint(50,500),\n",
    "              'max_depth': randint(1,20)}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rand_search = RandomizedSearchCV(rf, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = rand_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad8b29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d68eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea49ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #optional extra\n",
    "# # Checking to see how the data was scaled\n",
    "\n",
    "# # Visualize histograms before scaling for 'Fe' (example)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(X['Fe'], bins=50, color='blue', alpha=0.7)\n",
    "# plt.title('Before Scaling: Fe')\n",
    "\n",
    "# # Visualize histograms after scaling for 'Fe'\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(X_scaled[:, X.columns.get_loc('Fe')], bins=50, color='green', alpha=0.7)\n",
    "# plt.title('After Scaling: Fe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d2ffd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #optional extra\n",
    "# Feature Importance\n",
    "# feature_importances = best_rf.feature_importances_\n",
    "# feature_importance_df = pd.DataFrame({'Element': elements, 'Importance': feature_importances})\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x='Importance', y='Element', data=feature_importance_df, color='steelblue')\n",
    "# plt.title('Feature Importance for Predicting Class A vs Class B')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989eba46",
   "metadata": {},
   "source": [
    "### Aim 2: Predict labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f2f5e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unlabeled_data_copy = unlabeled_data.copy()\n",
    "\n",
    "# Predict unlabeled data \n",
    "X_new = unlabeled_data[elements]\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "unlabeled_data.loc[:, 'Predicted_Class'] = le.inverse_transform(best_rf.predict(X_new_scaled))\n",
    "\n",
    "print(\"Predictions on Unlabeled Data:\")\n",
    "print(unlabeled_data[['Unique_ID', 'holeid', 'Predicted_Class']].head(70))\n",
    "unlabeled_data.to_csv('./data/predictions_on_unlabeled_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23730f2b",
   "metadata": {},
   "source": [
    "Results available on: predictions_on_unlabeled_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where the original 'Class' column is '?'\n",
    "unlabeled_data_question = unlabeled_data[unlabeled_data['Class'] == '?']\n",
    "\n",
    "# Count how many were predicted to be 'A' and 'B'\n",
    "class_counts_from_question = unlabeled_data_question['Predicted_Class'].value_counts()\n",
    "\n",
    "# Print out the prediction counts for 'A' and 'B' from '?'\n",
    "print(f\"Predictions for '?' class:\")\n",
    "print(class_counts_from_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb795e5",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcc086",
   "metadata": {},
   "source": [
    "- The model performs well overall, with high accuracy and strong performance for Class A.\n",
    "- Class B predictions are less accurate.\n",
    "- The imbalance in the dataset (more Class A samples) is probably why the model performs better for Class A \n",
    "- Pb has the highest feature importance (makes sense as this is a lead deposit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc4640",
   "metadata": {},
   "source": [
    "### Improvements: \n",
    "- Handle missing values more effectively (instead of dropping).\n",
    "- Perform additional model validation (e.g., cross-validation, ROC curves).\n",
    "- Experiment with other models (e.g., SVM, Gradient Boosting) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cabd73e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
